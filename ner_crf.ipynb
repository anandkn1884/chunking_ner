{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition using Conditional Random Fields.\n",
    "\n",
    "# Features have been chosen and multiple models have been created incrementally to test the accuracy.\n",
    "## 1. Pos tag of the current, previous and next word.\n",
    "## 2. Pos tag and Chunk tag of the current, previous and next word.\n",
    "## 3. Pos tag and Case check of the first character (bool), Digit check (bool) for each word- current, previous and next word.\n",
    "\n",
    "## Note : Other features can be chosen to test the model but this currently I have restricted the model to these three set of features.\n",
    "\n",
    "\n",
    "# Requires Sklearn CRF Suite, NLTK and Sklearn.\n",
    "# Created using Pycharm. Add package sklearn-crfsuite.\n",
    "\n",
    "# Create new models by specifying new_model name and running the train_model() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn_crfsuite\n",
    "import nltk\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide model_name to save\n",
    "# model_name = 'ner_crf_model_pos'\n",
    "# model_name = 'ner_crf_model_pos_chunk'\n",
    "model_name = 'ner_crf_model_pos_chunk_case_digit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide chunking model name\n",
    "chunking_model_name = 'chunking_crf_model_pos_next_previous_word_case_start_word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New model name\n",
    "new_model = 'ner_crf_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentence count.\n",
    "def get_sentence_count(filepath):\n",
    "    count = 0\n",
    "    with open(filepath) as chunking_data:\n",
    "        for line in chunking_data.readlines():\n",
    "            if 0 == len(line.strip()):\n",
    "                count += 1\n",
    "    print ('Total number of sentences : ', count)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test and train split given filepath.\n",
    "def get_sentences_from_file(filepath, tt_division=0.7):\n",
    "    sentence = []\n",
    "    train = []\n",
    "    test = []\n",
    "    count = 0\n",
    "    sentence_count = get_sentence_count(filepath)\n",
    "    sentence_count = sentence_count * tt_division\n",
    "\n",
    "    with open(filepath) as chunking_data:\n",
    "        for line in chunking_data.readlines():\n",
    "            if 'DOCSTART' in line:\n",
    "                continue\n",
    "            if 0 == len(line.strip()):\n",
    "                if count <= sentence_count:\n",
    "                    train.append(sentence)\n",
    "                else:\n",
    "                    test.append(sentence)\n",
    "                count += 1\n",
    "                sentence = []\n",
    "                continue\n",
    "            else:\n",
    "                parts = line.split(' ')\n",
    "                temp = (parts[0].strip(), parts[1].strip(), parts[2].strip(), parts[3].strip())\n",
    "\n",
    "            sentence.append(temp)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features : POS TAG and CHUNK TAG of the current word\n",
    "def word2features_pos_chunk(sentence, i):\n",
    "    pos = sentence[i][1]\n",
    "    chunk = sentence[i][2]\n",
    "    features = {\n",
    "        'pos': pos,\n",
    "        'chunk': chunk,\n",
    "    }\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - POS\n",
    "# Previous and Next Word\n",
    "# Previous word Next Word POS tag\n",
    "def word2features_pos(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = {\n",
    "        'postag': postag,\n",
    "    }\n",
    "\n",
    "    # Get previous word and pos tag\n",
    "    if i > 0:\n",
    "        previous_word = sent[i-1][0]\n",
    "        previous_postag = sent[i-1][1]\n",
    "        features.update({\n",
    "            'previous_postag': previous_postag,\n",
    "        })\n",
    "    else:\n",
    "        features['Start'] = True\n",
    "\n",
    "    # Get next word and pos tag\n",
    "    if i < len(sent)-1:\n",
    "        next_word = sent[i+1][0]\n",
    "        next_postag = sent[i+1][1]\n",
    "        features.update({\n",
    "            'next_postag': next_postag,\n",
    "        })\n",
    "    else:\n",
    "        features['End'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - POS\n",
    "# Previous and Next Word\n",
    "# Previous word Next Word POS tag\n",
    "# Upper case bool for first letter of token\n",
    "# Is number/digit bool\n",
    "def word2features_pos_chunk_case_digit(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    chunktag = sent[i][2]\n",
    "    features = {\n",
    "        'isupper': word[0].isupper(),\n",
    "        'isdigit': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'chunktag': chunktag,\n",
    "    }\n",
    "    if i > 0:\n",
    "        previous_word = sent[i-1][0]\n",
    "        previous_tag = sent[i-1][1]\n",
    "        previous_chunktag = sent[i - 1][2]\n",
    "        features.update({\n",
    "            'previous_isupper': previous_word[0].isupper(),\n",
    "            'previous_isdigit': previous_word.isdigit(),\n",
    "            'previous_postag': previous_tag,\n",
    "            'previous_chunktag': previous_chunktag,\n",
    "        })\n",
    "    else:\n",
    "        features['Start'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        next_word = sent[i+1][0]\n",
    "        next_postag = sent[i+1][1]\n",
    "        next_chunktag = sent[i + 1][2]\n",
    "        features.update({\n",
    "            'next_isupper()': next_word[0].isupper(),\n",
    "            'next_isdigit': next_word.isdigit(),\n",
    "            'next_postag': next_postag,\n",
    "            'next_chunktag': next_chunktag,\n",
    "        })\n",
    "    else:\n",
    "        features['End'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Start Chunking Methods #############################\n",
    "# Features - POS\n",
    "# Previous word Next Word POS tag\n",
    "# Previous and Next Word Case\n",
    "def chunking_word2features_pos_next_previous_word_case(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = {\n",
    "        'isupper': word[0].isupper(),\n",
    "        'postag': postag,\n",
    "    }\n",
    "\n",
    "    # Get previous word and pos tag\n",
    "    if i > 0:\n",
    "        previous_word = sent[i-1][0]\n",
    "        previous_postag = sent[i-1][1]\n",
    "        features.update({\n",
    "            'isupper': previous_word[0].isupper(),\n",
    "            'previous_postag': previous_postag,\n",
    "        })\n",
    "    else:\n",
    "        features['Start'] = True\n",
    "\n",
    "    # Get next word and pos tag\n",
    "    if i < len(sent)-1:\n",
    "        next_word = sent[i+1][0]\n",
    "        next_postag = sent[i+1][1]\n",
    "        features.update({\n",
    "            'isupper': next_word[0].isupper(),\n",
    "            'next_postag': next_postag,\n",
    "        })\n",
    "    else:\n",
    "        features['End'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "# Get features for each word in the sentence.\n",
    "def getSentencefeatures_for_chunking(sentence):\n",
    "    return [chunking_word2features_pos_next_previous_word_case(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "# Predicts on list of text.\n",
    "# Returns Chunks from the sentence and and Predicted labels.\n",
    "def generate_chunks(text_list):\n",
    "\n",
    "    # Generate POS Tag for text list\n",
    "    test_data = nltk.pos_tag(text_list)\n",
    "\n",
    "    # Get Test sentences with features for sentence.\n",
    "    # X_test = [getSentencefeatures(s) for s in test_data]\n",
    "    X_test = [getSentencefeatures_for_chunking(test_data)]\n",
    "\n",
    "    try:\n",
    "        clf = joblib.load(chunking_model_name)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        chunks = []\n",
    "        chunk = ''\n",
    "        tag_index = 0\n",
    "        for i in range(len(y_pred[0])):\n",
    "            if tag_index >= len(y_pred[0]):\n",
    "                break\n",
    "            if y_pred[0][tag_index].startswith('O'):\n",
    "                chunks[len(chunks)-1] = chunks[len(chunks)-1] + text_list[tag_index]\n",
    "                break\n",
    "            if y_pred[0][tag_index].startswith('B-'):\n",
    "                chunk += text_list[tag_index] + ' '\n",
    "                next_tag_index = tag_index + 1\n",
    "                for j in range(len(y_pred[0])):\n",
    "                    if next_tag_index >= len(y_pred[0]):\n",
    "                        chunks.append(chunk)\n",
    "                        chunk = ''\n",
    "                        tag_index = next_tag_index\n",
    "                        break\n",
    "                    if y_pred[0][next_tag_index].startswith('I-'):\n",
    "                        chunk += text_list[next_tag_index] + ' '\n",
    "                        next_tag_index = next_tag_index + 1\n",
    "                    else:\n",
    "                        chunks.append(chunk)\n",
    "                        chunk = ''\n",
    "                        tag_index = next_tag_index\n",
    "                        break\n",
    "\n",
    "        return chunks, test_data, y_pred\n",
    "    except 'AttributeError':\n",
    "        print('There was an exception!', sys.exc_info()[0])\n",
    "\n",
    "# Merges the text list with the chunk tags\n",
    "def merge_with_chunk_tags(text_list, chunk_tags):\n",
    "    for i in range(len(text_list)):\n",
    "        text_list[i] = (text_list[i][0], text_list[i][1], chunk_tags[0][i])\n",
    "\n",
    "    return text_list\n",
    "\n",
    "############################# End Chunking Methods #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features for each word in the sentence.\n",
    "def getSentencefeatures(sentence):\n",
    "    return [word2features_pos_chunk_case_digit(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "# Get lables for each word in the sentence\n",
    "def getlabels(sentence):\n",
    "    return [label for token, postag, chunktag, label in sentence]\n",
    "\n",
    "# Get tokens from the sentence\n",
    "def sent2tokens(sentence):\n",
    "    return [token for token, postag, chunktag, label in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positives\n",
    "# B-* => B-*\n",
    "# I-* => I-*\n",
    "#\n",
    "# True Negatives\n",
    "# O => O\n",
    "#\n",
    "# False Negatives\n",
    "# B-* => O\n",
    "# I-* => O\n",
    "#\n",
    "# False Positives\n",
    "# O => B-*\n",
    "# O => I-*\n",
    "def compute_results(y_pred, y_test):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_test[i])):\n",
    "            if y_test[i][j].startswith(('B-', 'I-')) and y_pred[i][j].startswith(('B-', 'I-')):\n",
    "                tp += 1\n",
    "            if y_test[i][j].startswith('O') and y_pred[i][j].startswith('O'):\n",
    "                tn += 1\n",
    "            if y_test[i][j].startswith(('B-', 'I-')) and y_pred[i][j].startswith('O') :\n",
    "                fn += 1\n",
    "            if y_test[i][j].startswith('O') and y_pred[i][j].startswith(('B-', 'I-')):\n",
    "                fp += 1\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "    return precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts on input test file based on a trained model.\n",
    "# Specify model file name\n",
    "def predict_on_test_set(filepath):\n",
    "    sentence = []\n",
    "    test_data = []\n",
    "    count = 0\n",
    "    with open(filepath) as chunking_data:\n",
    "        for line in chunking_data.readlines():\n",
    "\n",
    "            if 0 == len(line.strip()):\n",
    "                test_data.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            else:\n",
    "                parts = line.split(' ')\n",
    "                temp = (parts[0].strip(), parts[1].strip(), parts[2].strip(), parts[3].strip())\n",
    "\n",
    "            sentence.append(temp)\n",
    "            temp = ()\n",
    "\n",
    "    # Get Test sentences with features.\n",
    "    X_test = [getSentencefeatures(s) for s in test_data]\n",
    "\n",
    "    # Get Test lables.\n",
    "    y_test = [getlabels(s) for s in test_data]\n",
    "\n",
    "    try:\n",
    "        clf = joblib.load(model_name)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        precision, recall, accuracy = compute_results(y_pred, y_test)\n",
    "        print ('\\n Model Name :', model_name.title(), '\\nPrecision:', precision , '\\nRecall: ', recall, '\\nAccuracy:', accuracy)\n",
    "        labels = list(clf.classes_)\n",
    "        metrics.flat_f1_score(y_test, y_pred,\n",
    "                              average='weighted', labels=labels)\n",
    "\n",
    "        # Sort and Group labels\n",
    "        sorted_labels = sorted(\n",
    "            labels,\n",
    "            key=lambda name: (name[1:], name[0])\n",
    "        )\n",
    "\n",
    "        print('\\nResults from sklearn_crfsuite metrics \\n')\n",
    "        print(metrics.flat_classification_report(\n",
    "            y_test, y_pred, labels=sorted_labels, digits=3\n",
    "        ))\n",
    "    except:\n",
    "        print('There was an exception!',  sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new model based on training file or if there is a change in the feature set\n",
    "def train_model():\n",
    "    # Provide input file name\n",
    "    trainfile = 'ner_dataset.txt'\n",
    "\n",
    "    # Use 95% of the data for training (can be more or less)\n",
    "    train_sentences, test_sentences = get_sentences_from_file(trainfile, 0.95)\n",
    "\n",
    "    print('Number of training sentences : ', len(train_sentences))\n",
    "    print('Number of test sentences : ', len(test_sentences))\n",
    "\n",
    "    # Get Training sentence with features.\n",
    "    X_train = [getSentencefeatures(s) for s in train_sentences]\n",
    "\n",
    "    # Get Training lables\n",
    "    y_train = [getlabels(s) for s in train_sentences]\n",
    "\n",
    "    # Get Test sentences with features.\n",
    "    X_test = [getSentencefeatures(s) for s in test_sentences]\n",
    "\n",
    "    # Get Test lables\n",
    "    y_test = [getlabels(s) for s in test_sentences]\n",
    "\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "\n",
    "    crf.fit(X_train, y_train)\n",
    "\n",
    "    joblib.dump(crf, new_model)\n",
    "    \n",
    "    print('\\nModel created : ', new_model)\n",
    "    \n",
    "    labels = list(crf.classes_)\n",
    "\n",
    "    y_pred = crf.predict(X_test)\n",
    "\n",
    "    precision, recall, accuracy = compute_results(y_pred, y_test)\n",
    "\n",
    "    print ('\\nPrecision :', precision, '\\nRecall : ', recall, '\\nAccuracy : ', accuracy)\n",
    "\n",
    "    metrics.flat_f1_score(y_test, y_pred,\n",
    "                          average='weighted', labels=labels)\n",
    "\n",
    "    # group B and I results\n",
    "    sorted_labels = sorted(\n",
    "        labels,\n",
    "        key=lambda name: (name[1:], name[0])\n",
    "    )\n",
    "\n",
    "    print('\\nResults from sklearn_crfsuite metrics \\n')\n",
    "\n",
    "    print(metrics.flat_classification_report(\n",
    "        y_test, y_pred, labels=sorted_labels, digits=3\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts on list of text.\n",
    "# Returns Chunks from the sentence and and Predicted labels.\n",
    "def predicts_on_text(text_list):\n",
    "\n",
    "    # Generate Chunks based on the chunking crf model\n",
    "    chunks, test_data, chunk_tags = generate_chunks(text_list)\n",
    "\n",
    "    test_data = merge_with_chunk_tags(test_data, chunk_tags)\n",
    "\n",
    "    # Get Test sentences with features for sentence.\n",
    "    # X_test = [getSentencefeatures(s) for s in test_data]\n",
    "    X_test = [getSentencefeatures(test_data)]\n",
    "\n",
    "    try:\n",
    "        clf = joblib.load(model_name)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        print ('Chunks: \\n')\n",
    "        chunks = []\n",
    "        chunk = ''\n",
    "        tag_index = 0\n",
    "        for i in range(len(y_pred[0])):\n",
    "            if tag_index >= len(y_pred[0]):\n",
    "                break\n",
    "            # if y_pred[0][tag_index].startswith('O'):\n",
    "            #     chunks[len(chunks)-1] = chunks[len(chunks)-1] + text_list[tag_index]\n",
    "            #     break\n",
    "            if y_pred[0][tag_index].startswith('B-'):\n",
    "                chunk += text_list[tag_index] + ' '\n",
    "                next_tag_index = tag_index + 1\n",
    "                for j in range(len(y_pred[0])):\n",
    "                    if next_tag_index >= len(y_pred[0]):\n",
    "                        chunks.append(chunk.strip())\n",
    "                        chunk = ''\n",
    "                        tag_index = next_tag_index\n",
    "                        break\n",
    "                    if y_pred[0][next_tag_index].startswith('I-'):\n",
    "                        chunk += text_list[next_tag_index] + ' '\n",
    "                        next_tag_index = next_tag_index + 1\n",
    "                    else:\n",
    "                        chunks.append(chunk.strip())\n",
    "                        chunk = ''\n",
    "                        tag_index = next_tag_index\n",
    "                        break\n",
    "            else:\n",
    "                tag_index += 1\n",
    "\n",
    "        for c in chunks:\n",
    "            print(c)\n",
    "\n",
    "        return chunks, y_pred\n",
    "    except 'AttributeError':\n",
    "        print('There was an exception!', sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences :  18453\n",
      "Number of training sentences :  17531\n",
      "Number of test sentences :  922\n",
      "\n",
      "Model created :  ner_crf_model\n",
      "\n",
      "Precision : 0.8971261974177426 \n",
      "Recall :  0.8647129666800482 \n",
      "Accuracy :  0.9626932413440654\n",
      "\n",
      "Results from sklearn_crfsuite metrics \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.975     0.981     0.978     13163\n",
      "       B-LOC      0.505     0.437     0.468       561\n",
      "       I-LOC      0.295     0.210     0.245        62\n",
      "      B-MISC      0.733     0.611     0.667       373\n",
      "      I-MISC      0.457     0.372     0.410       129\n",
      "       B-ORG      0.474     0.424     0.448       323\n",
      "       I-ORG      0.317     0.526     0.395       196\n",
      "       B-PER      0.653     0.665     0.659       493\n",
      "       I-PER      0.759     0.729     0.744       354\n",
      "\n",
      "   micro avg      0.912     0.912     0.912     15654\n",
      "   macro avg      0.574     0.551     0.557     15654\n",
      "weighted avg      0.911     0.912     0.911     15654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Name : Ner_Crf_Model_Pos_Chunk_Case_Digit \n",
      "Precision: 0.886762360446571 \n",
      "Recall:  0.8348348348348348 \n",
      "Accuracy: 0.9632860040567951\n",
      "\n",
      "Results from sklearn_crfsuite metrics \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.974     0.983     0.979      4264\n",
      "       B-LOC      0.536     0.462     0.496       210\n",
      "       I-LOC      0.267     0.211     0.235        19\n",
      "      B-MISC      0.771     0.711     0.740       114\n",
      "      I-MISC      0.588     0.417     0.488        24\n",
      "       B-ORG      0.323     0.247     0.280        81\n",
      "       I-ORG      0.400     0.494     0.442        77\n",
      "       B-PER      0.535     0.552     0.544        96\n",
      "       I-PER      0.604     0.711     0.653        45\n",
      "\n",
      "   micro avg      0.918     0.918     0.918      4930\n",
      "   macro avg      0.555     0.532     0.540      4930\n",
      "weighted avg      0.915     0.918     0.916      4930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_on_test_set('ner_test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: \n",
      "\n",
      "India\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['India'], [['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = ['India', 'is','playing','in','the','world','cup','.']\n",
    "predicts_on_text(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: \n",
      "\n",
      "England\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['England'], [['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = ['England', 'is','playing','in','the','world','cup','.']\n",
    "predicts_on_text(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: \n",
      "\n",
      "United Kingdom\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['United Kingdom'], [['B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = ['United', 'Kingdom', 'is','playing','in','the','world','cup','.']\n",
    "predicts_on_text(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: \n",
      "\n",
      "India\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['India'], [['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = ['India', 'is','playing','in','the','world','cup','.']\n",
    "predicts_on_text(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
