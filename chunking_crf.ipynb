{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking using Conditional Random Fields.\n",
    "\n",
    "# Features have been chosen and multiple models have been created incrementally to test the accuracy.\n",
    "## 1. Only the Pos tag of the current word.\n",
    "## 2. Pos tag of the current, previous and next word.\n",
    "## 3. Pos tag and Case of the first character of each word - current, previous and next word.\n",
    "\n",
    "## Note : Other features can be chosen to test the model but this currently I have restricted the model to these three set of features.\n",
    "\n",
    "\n",
    "# Requires Sklearn CRF Suite, NLTK and Sklearn.\n",
    "# Created using Pycharm. Add package sklearn-crfsuite.\n",
    "\n",
    "## Note: Tried to work with Word Embeddings as feature using a word2vec model. Was getting some errors from the CRF Suite in the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn_crfsuite\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose any of the following models for\n",
    "# model_name = 'chunking_crf_model_pos'\n",
    "# model_name = 'chunking_crf_model_pos_next_previous_word'\n",
    "model_name = 'chunking_crf_model_pos_next_previous_word_case_start_word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Model file name if new features are chosen.\n",
    "# Use train_model() method to train new model.\n",
    "new_model_name = 'chunking_crf_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec model name for work embeddings\n",
    "w2v_model_name = 'chunking_w2v_word_embedding_model'\n",
    "w2v_model = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentence count.\n",
    "def get_sentence_count(filepath):\n",
    "    count = 0\n",
    "    with open(filepath) as chunking_data:\n",
    "        for line in chunking_data.readlines():\n",
    "            if 0 == len(line.strip()):\n",
    "                count += 1\n",
    "    print ('Total Number of Sentences : ', count)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method used to split the data set and\n",
    "# get test and train split given filepath.\n",
    "def get_test_train_split_from_file(filepath, tt_division=0.7):\n",
    "    sentence = []\n",
    "    train = []\n",
    "    test = []\n",
    "    count = 0\n",
    "    sentence_count = get_sentence_count(filepath)\n",
    "    sentence_count = sentence_count * tt_division\n",
    "\n",
    "    with open(filepath) as chunking_data:\n",
    "        for line in chunking_data.readlines():\n",
    "\n",
    "            if 0 == len(line.strip()):\n",
    "                if count <= sentence_count:\n",
    "                    train.append(sentence)\n",
    "                else:\n",
    "                    test.append(sentence)\n",
    "                count += 1\n",
    "                sentence = []\n",
    "                continue\n",
    "            else:\n",
    "                parts = line.split(' ')\n",
    "                temp = (parts[0].strip(), parts[1].strip(), parts[2].strip())\n",
    "\n",
    "            sentence.append(temp)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - POS\n",
    "def word2features_pos(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = {\n",
    "        'postag': postag,\n",
    "    }\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - POS\n",
    "# Previous word Next Word POS tag\n",
    "def word2features_pos_next_previous_word(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = {\n",
    "        'postag': postag,\n",
    "    }\n",
    "\n",
    "    # Get previous word and pos tag\n",
    "    if i > 0:\n",
    "        previous_word = sent[i-1][0]\n",
    "        previous_postag = sent[i-1][1]\n",
    "        features.update({\n",
    "            'previous_postag': previous_postag,\n",
    "        })\n",
    "    else:\n",
    "        features['Start'] = True\n",
    "\n",
    "    # Get next word and pos tag\n",
    "    if i < len(sent)-1:\n",
    "        next_word = sent[i+1][0]\n",
    "        next_postag = sent[i+1][1]\n",
    "        features.update({\n",
    "            'next_postag': next_postag,\n",
    "        })\n",
    "    else:\n",
    "        features['End'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features - POS\n",
    "# Previous word Next Word POS tag\n",
    "# Previous and Next Word Case\n",
    "def word2features_pos_next_previous_word_case(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = {\n",
    "        'isupper': word[0].isupper(),\n",
    "        'postag': postag,\n",
    "    }\n",
    "\n",
    "    # Get previous word and pos tag\n",
    "    if i > 0:\n",
    "        previous_word = sent[i-1][0]\n",
    "        previous_postag = sent[i-1][1]\n",
    "        features.update({\n",
    "            'isupper': previous_word[0].isupper(),\n",
    "            'previous_postag': previous_postag,\n",
    "        })\n",
    "    else:\n",
    "        features['Start'] = True\n",
    "\n",
    "    # Get next word and pos tag\n",
    "    if i < len(sent)-1:\n",
    "        next_word = sent[i+1][0]\n",
    "        next_postag = sent[i+1][1]\n",
    "        features.update({\n",
    "            'isupper': next_word[0].isupper(),\n",
    "            'next_postag': next_postag,\n",
    "        })\n",
    "    else:\n",
    "        features['End'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return Word vector if one exists, else return None\n",
    "def word2vec_feature(word):\n",
    "    feature = ('None')\n",
    "    try:\n",
    "        feature = (w2v_model.wv[word])\n",
    "    except KeyError:\n",
    "        feature = ('None')\n",
    "\n",
    "    return feature\n",
    "\n",
    "# Features - Word 2 vec word embeddings\n",
    "def word2features_w2v(sent, i):\n",
    "    word = sent[i][0]\n",
    "    features = {\n",
    "        'w2v': word2vec_feature(word),\n",
    "    }\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features for each word in the sentence.\n",
    "def getSentencefeatures(sentence):\n",
    "    return [word2features_pos_next_previous_word_case(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "# Get lables for each word in the sentence\n",
    "def getlabels(sentence):\n",
    "    return [label for token, postag, label in sentence]\n",
    "\n",
    "# Get tokens from the sentence\n",
    "def sent2tokens(sentence):\n",
    "    return [token for token, postag, label in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TP\n",
    "# B-* => B-*\n",
    "# I-* => I-*\n",
    "#\n",
    "# TN\n",
    "# O => O\n",
    "#\n",
    "# FN\n",
    "# B-* => O\n",
    "# I-* => O\n",
    "#\n",
    "# FP\n",
    "# O => B-*\n",
    "# O => I-*\n",
    "def compute_results(y_pred, y_test):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_test[i])):\n",
    "\n",
    "            if y_test[i][j].startswith(('B-', 'I-')) and y_pred[i][j].startswith(('B-', 'I-')):\n",
    "                tp += 1\n",
    "            if y_test[i][j].startswith('O') and y_pred[i][j].startswith('O'):\n",
    "                tn += 1\n",
    "            if y_test[i][j].startswith('O') and y_pred[i][j].startswith(('B-', 'I-')):\n",
    "                fp += 1\n",
    "            if y_test[i][j].startswith(('B-', 'I-')) and y_pred[i][j].startswith('O') :\n",
    "                fn += 1\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "\n",
    "    return precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts on input test file based on a trained model.\n",
    "# Specify model_name file\n",
    "def predict_on_test_set(filepath):\n",
    "    sentence = []\n",
    "    test_data = []\n",
    "    count = 0\n",
    "    with open(filepath) as chunking_data:\n",
    "        for line in chunking_data.readlines():\n",
    "\n",
    "            if 0 == len(line.strip()):\n",
    "                test_data.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            else:\n",
    "                parts = line.split(' ')\n",
    "                temp = (parts[0].strip(), parts[1].strip(), parts[2].strip())\n",
    "\n",
    "            sentence.append(temp)\n",
    "            temp = ()\n",
    "\n",
    "    # Get Test sentences with features.\n",
    "    X_test = [getSentencefeatures(s) for s in test_data]\n",
    "\n",
    "    # Get Test lables.\n",
    "    y_test = [getlabels(s) for s in test_data]\n",
    "\n",
    "    try:\n",
    "        clf = joblib.load(model_name)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        precision, recall, accuracy = compute_results(y_pred, y_test)\n",
    "        print ('\\nModel Name :', model_name.title(), '\\nPrecision:', precision , '\\nRecall: ', recall, '\\nAccuracy:', accuracy)\n",
    "        labels = list(clf.classes_)\n",
    "        metrics.flat_f1_score(y_test, y_pred,\n",
    "                              average='weighted', labels=labels)\n",
    "\n",
    "        # Sort and Group labels\n",
    "        sorted_labels = sorted(\n",
    "            labels,\n",
    "            key=lambda name: (name[1:], name[0])\n",
    "        )\n",
    "\n",
    "        print('\\nResults from sklearn_crfsuite metrics \\n')\n",
    "        print(metrics.flat_classification_report(\n",
    "            y_test, y_pred, labels=sorted_labels, digits=3\n",
    "        ))\n",
    "    except:\n",
    "        print('There was an exception!',  sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts on list of text.\n",
    "# Returns Chunks from the sentence and and Predicted labels.\n",
    "def predicts_on_text(text_list):\n",
    "\n",
    "    # Generate POS Tag for text list\n",
    "    test_data = nltk.pos_tag(text_list)\n",
    "\n",
    "    # Get Test sentences with features for sentence.\n",
    "    # X_test = [getSentencefeatures(s) for s in test_data]\n",
    "    X_test = [getSentencefeatures(test_data)]\n",
    "\n",
    "    try:\n",
    "        clf = joblib.load(model_name)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        print ('Chunks: \\n')\n",
    "        chunks = []\n",
    "        chunk = ''\n",
    "        tag_index = 0\n",
    "        for i in range(len(y_pred[0])):\n",
    "            if tag_index >= len(y_pred[0]):\n",
    "                break\n",
    "            if y_pred[0][tag_index].startswith('O'):\n",
    "                chunks[len(chunks)-1] = chunks[len(chunks)-1] + text_list[tag_index]\n",
    "                break\n",
    "            if y_pred[0][tag_index].startswith('B-'):\n",
    "                chunk += text_list[tag_index] + ' '\n",
    "                next_tag_index = tag_index + 1\n",
    "                for j in range(len(y_pred[0])):\n",
    "                    if next_tag_index >= len(y_pred[0]):\n",
    "                        chunks.append(chunk)\n",
    "                        chunk = ''\n",
    "                        tag_index = next_tag_index\n",
    "                        break\n",
    "                    if y_pred[0][next_tag_index].startswith('I-'):\n",
    "                        chunk += text_list[next_tag_index] + ' '\n",
    "                        next_tag_index = next_tag_index + 1\n",
    "                    else:\n",
    "                        chunks.append(chunk)\n",
    "                        chunk = ''\n",
    "                        tag_index = next_tag_index\n",
    "                        break\n",
    "\n",
    "        for c in chunks:\n",
    "            print(c)\n",
    "\n",
    "        return chunks, y_pred\n",
    "    except 'AttributeError':\n",
    "        print('There was an exception!', sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec model for word embeddings.\n",
    "def train_word2vec_model_for_word_embeddings(filepath):\n",
    "    count = get_sentence_count(filepath)\n",
    "    sentences = [None] * count\n",
    "    sentence = []\n",
    "    index = 0\n",
    "    with open(filepath) as chunking_data:\n",
    "        for line in chunking_data.readlines():\n",
    "\n",
    "            if 0 == len(line.strip()):\n",
    "                sentences[index] = sentence\n",
    "                index += 1\n",
    "                sentence = []\n",
    "                continue\n",
    "            else:\n",
    "                parts = line.split(' ')\n",
    "                temp = parts[0].strip()\n",
    "\n",
    "            sentence.append(temp)\n",
    "\n",
    "    w2v_model = Word2Vec(sentences, size=128, window=5, min_count=3, workers=4)\n",
    "\n",
    "    # Save w2v model.\n",
    "    joblib.dump(w2v_model, w2v_model_name)\n",
    "\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new model based on training file or if there is a change in the feature set\n",
    "def train_model():\n",
    "    # Provide input file name\n",
    "    trainfile = 'chunking_dataset.txt'\n",
    "\n",
    "    train_sentences, test_sentences = get_test_train_split_from_file(trainfile, 0.95)\n",
    "\n",
    "    print('Number of Training Sentences : ', len(train_sentences))\n",
    "    print('Number of Test Sentences : ', len(test_sentences))\n",
    "\n",
    "    # Get Training sentence with features.\n",
    "    X_train = [getSentencefeatures(s) for s in train_sentences]\n",
    "\n",
    "    # Get Training lables.\n",
    "    y_train = [getlabels(s) for s in train_sentences]\n",
    "\n",
    "    # Get Test sentences with features.\n",
    "    X_test = [getSentencefeatures(s) for s in test_sentences]\n",
    "\n",
    "    # Get Test lables.\n",
    "    y_test = [getlabels(s) for s in test_sentences]\n",
    "\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True,\n",
    "    )\n",
    "\n",
    "    crf.fit(X_train, y_train)\n",
    "\n",
    "    labels = list(crf.classes_)\n",
    "\n",
    "    # Create a CRF model file\n",
    "    joblib.dump(crf, new_model_name)\n",
    "\n",
    "    y_pred = crf.predict(X_test)\n",
    "\n",
    "    precision, recall, accuracy = compute_results(y_pred, y_test)\n",
    "\n",
    "    print ('\\nPrecision :', precision, '\\nRecall : ', recall, '\\nAccuracy : ', accuracy)\n",
    "\n",
    "    metrics.flat_f1_score(y_test, y_pred,\n",
    "                          average='weighted', labels=labels)\n",
    "\n",
    "    # group B and I results\n",
    "    sorted_labels = sorted(\n",
    "        labels,\n",
    "        key=lambda name: (name[1:], name[0])\n",
    "    )\n",
    "\n",
    "    print(metrics.flat_classification_report(\n",
    "        y_test, y_pred, labels=sorted_labels, digits=3\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Sentences :  8936\n",
      "Number of Training Sentences :  8490\n",
      "Number of Test Sentences :  446\n",
      "{'isupper': False, 'postag': 'NN', 'Start': True, 'next_postag': 'IN'}\n",
      "\n",
      "Precision : 0.9925825323861261 \n",
      "Recall :  0.9947649460789446 \n",
      "Accuracy :  0.9889558232931727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.964     0.949     0.957      1405\n",
      "      B-ADJP      0.712     0.619     0.662        84\n",
      "      I-ADJP      0.609     0.824     0.700        17\n",
      "      B-ADVP      0.720     0.768     0.743       164\n",
      "      I-ADVP      0.417     0.357     0.385        14\n",
      "     B-CONJP      0.000     0.000     0.000         0\n",
      "     I-CONJP      0.000     0.000     0.000         0\n",
      "      B-INTJ      0.000     0.000     0.000         0\n",
      "      I-INTJ      0.000     0.000     0.000         0\n",
      "       B-LST      0.000     0.000     0.000         0\n",
      "        B-NP      0.962     0.946     0.954      2904\n",
      "        I-NP      0.949     0.960     0.955      3459\n",
      "        B-PP      0.877     0.952     0.913      1141\n",
      "        I-PP      0.000     0.000     0.000        14\n",
      "       B-PRT      0.476     0.417     0.444        24\n",
      "       I-PRT      0.000     0.000     0.000         0\n",
      "      B-SBAR      0.788     0.241     0.369       108\n",
      "      I-SBAR      1.000     0.500     0.667         2\n",
      "       B-UCP      0.000     0.000     0.000         0\n",
      "       I-UCP      0.000     0.000     0.000         0\n",
      "        B-VP      0.934     0.953     0.944      1028\n",
      "        I-VP      0.937     0.936     0.937       592\n",
      "\n",
      "   micro avg      0.936     0.936     0.936     10956\n",
      "   macro avg      0.470     0.428     0.438     10956\n",
      "weighted avg      0.935     0.936     0.934     10956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Name : Chunking_Crf_Model_Pos_Next_Previous_Word_Case_Start_Word \n",
      "Precision: 0.9936428433827779 \n",
      "Recall:  0.993260157904872 \n",
      "Accuracy: 0.9885135135135135\n",
      "\n",
      "Results from sklearn_crfsuite metrics \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.952     0.955     0.953       727\n",
      "      B-ADJP      0.744     0.592     0.659        49\n",
      "      I-ADJP      0.692     0.900     0.783        10\n",
      "      B-ADVP      0.744     0.728     0.736        92\n",
      "      I-ADVP      0.200     0.143     0.167         7\n",
      "     B-CONJP      0.000     0.000     0.000         0\n",
      "     I-CONJP      0.000     0.000     0.000         0\n",
      "      B-INTJ      0.000     0.000     0.000         0\n",
      "      I-INTJ      0.000     0.000     0.000         0\n",
      "       B-LST      0.000     0.000     0.000         0\n",
      "        B-NP      0.962     0.960     0.961      1563\n",
      "        I-NP      0.958     0.957     0.957      1827\n",
      "        B-PP      0.857     0.947     0.900       588\n",
      "        I-PP      0.667     0.167     0.267        12\n",
      "       B-PRT      0.474     0.450     0.462        20\n",
      "       I-PRT      0.000     0.000     0.000         0\n",
      "      B-SBAR      1.000     0.222     0.364        72\n",
      "      I-SBAR      1.000     0.500     0.667         2\n",
      "       B-UCP      0.000     0.000     0.000         0\n",
      "       I-UCP      0.000     0.000     0.000         0\n",
      "        B-VP      0.929     0.962     0.945       608\n",
      "        I-VP      0.935     0.927     0.931       343\n",
      "\n",
      "   micro avg      0.935     0.935     0.935      5920\n",
      "   macro avg      0.505     0.428     0.443      5920\n",
      "weighted avg      0.936     0.935     0.932      5920\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predict_on_test_set('chunking_test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: \n",
      "\n",
      "The Food and Drug Administration \n",
      "had raised \n",
      "questions \n",
      "about \n",
      "the device ’s design .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['The Food and Drug Administration ',\n",
       "  'had raised ',\n",
       "  'questions ',\n",
       "  'about ',\n",
       "  'the device ’s design .'],\n",
       " [['B-NP',\n",
       "   'I-NP',\n",
       "   'I-NP',\n",
       "   'I-NP',\n",
       "   'I-NP',\n",
       "   'B-VP',\n",
       "   'I-VP',\n",
       "   'B-NP',\n",
       "   'B-PP',\n",
       "   'B-NP',\n",
       "   'I-NP',\n",
       "   'I-NP',\n",
       "   'I-NP',\n",
       "   'O']])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    text_list = ['The','Food','and','Drug','Administration','had','raised','questions','about','the','device', '’s','design', '.']\n",
    "    predicts_on_text(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: \n",
      "\n",
      "South African economy \n",
      "is \n",
      "a great example .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['South African economy ', 'is ', 'a great example .'],\n",
       " [['B-NP', 'I-NP', 'I-NP', 'B-VP', 'B-NP', 'I-NP', 'I-NP', 'O']])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = ['South', 'African', 'economy', 'is', 'a', 'great', 'example', '.']\n",
    "predicts_on_text(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: \n",
      "\n",
      "south african economy \n",
      "is \n",
      "a great example .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['south african economy ', 'is ', 'a great example .'],\n",
       " [['B-NP', 'I-NP', 'I-NP', 'B-VP', 'B-NP', 'I-NP', 'I-NP', 'O']])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = ['south', 'african', 'economy', 'is', 'a', 'great', 'example', '.']\n",
    "predicts_on_text(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
